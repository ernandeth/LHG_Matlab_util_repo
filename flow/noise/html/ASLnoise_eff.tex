
% This LaTeX was auto-generated from an M-file by MATLAB.
% To make changes, update the M-file and republish this document.

\documentclass{article}
\usepackage{graphicx}
\usepackage{color}

\sloppy
\definecolor{lightgray}{gray}{0.5}
\setlength{\parindent}{0pt}

\begin{document}

    
    \begin{verbatim}
% script to illustrate the properties of the different subtraction methods
% in ASL - turboCASL in event related experiments.
%
% uses simulated data with AR(1) noise
%
% 4 differencing matrices.
% calculates power, efficiency, MSE, bias of the beta estimate as a
% function of SNR
%

clear
doSpectra=0;
showPlots=1;
warning off
doNewMatrix=0;
doGLS=0;  % other wise it's OLS

rho=0.3;
TR=1.4;
nyq=1/(2*TR);
tlen=258;
NITER = 1;

% Make the differencing matrices
D1 = eye(tlen);
D2 = zeros(tlen/2,tlen);
for count=1:tlen/2
    D2(count,count*2-1)=1;
    D2(count,count*2)=-1;
end
D3 = zeros(tlen);
for count=1:tlen-1
    D3(count,count)=(-1)^(count-1);
    D3(count,count+1)=(-1)^(count);
end
D4 = zeros(tlen);
for count=1:tlen-2
    D4(count,count)=(-1)^(count-1);
    D4(count,count+1)=2*(-1)^(count);
    D4(count,count+2)=(-1)^(count-1);
end
% make up the sinc-interpolation matrix.
klen = 41;
t = linspace(-round(klen/2),round(klen/2),klen);
skernel = (sinc(t-0.5));
sk = zeros(1,length(skernel)*2);
sk(2:2:end) = skernel;
D5 = zeros(tlen + 2*klen);
for row=2:2:tlen+klen
    D5(row,row-1:row-2 + 2*klen ) = sk;

end
for row=3:2:tlen+klen
    D5(row,row-1:row-2 + 2*klen ) = -sk;
end

D5 = D5(klen:end-klen-1, 2*klen:end-klen+1);
for row=1:tlen
    % normalize the kernel
    D5(row,:) = D5(row,:) / abs(sum(D5(row,:)));
    % stick in the ones to represent the non-interpolated samples
    D5(row,row) = -(-1)^row;
end
D5 = D5(1:tlen, 1:tlen);

if (doNewMatrix==1)
    % Make the design matrix (main effect)
    X = zeros(tlen,1);
    %X(round([18:10:360]/1.4))=1;

    isi = rand(30,1)*12 + 5;
    isi = round(cumsum(isi));
    isi = isi(find(isi<tlen));
    X(isi)=1;


    X = conv(X,spm_hrf(TR));
    X = X(1:tlen);
    X = X/max(X);

    % modulation of the effect
    mm = ones(size(X));
    mm(1:2:end)=-1;
    X=X.*mm/2;

    % baseline perfusion: regressor for controls and regressor for tag:
    Xc = zeros(tlen,1);
    Xt = zeros(tlen,1);
    Xc(2:2:end)=1;
    Xt(1:2:end)=1;

    % put in BOLD effect regressor here:

    % baseline signal
    Xb = ones(size(Xc));

    % put it all together:
    % note that in the baseline case we use only the tag to prevent
    % too much colinearity with the activation vector
    X = [Xb Xc X];
else
    load /Users/hernan/matlab/flow/noise/ASLX.mat
end
B = [100 -1 -.5]'
signal = X*B ;


D5X = zeros(size(X));
for col=1:size(D5X,2)
    D5X(:,col) = sincsub(X(:,col));
end

if showPlots
    figure;  set(gcf,'Position',[1 1 250,450]);
    imagesc(X)
    colormap(gray)
    figure ,set(gcf,'Position',[1 1 450,450])
    plot(signal), title('signal'), drawnow
end

% determine a range of noise levels - SNRs..
nlevels = [0.2 0.5 1:2:10 20];
%nlevels = [0.5:0.5: 4];
%nlevels = 5;

%
Spower_SNR = zeros(5,length(nlevels));
eff_SNR = zeros(5,length(nlevels));
bias_SNR = zeros(5,length(nlevels));
std_SNR = zeros(5,length(nlevels));
bcrit_SNR = zeros(5,length(nlevels));
close all;

for nl=1:length(nlevels)
    fprintf('\rNoise level ...  %f ', nlevels(nl));

    noise_amp = nlevels(nl);

    simp = zeros(NITER, tlen/2);
    run = zeros(NITER,tlen-1);
    sinterp=zeros(NITER,tlen);
    orig=zeros(NITER,tlen);
    sincc=zeros(NITER,tlen);



    Bhat_raw = zeros(NITER,3);
    Bhat_simp = zeros(NITER,3);
    Bhat_run = zeros(NITER,3);
    Bhat_sur = zeros(NITER,3);
    Bhat_sinc = zeros(NITER,3);


    %%%
    %%% do the estimation for each case

    for iter=1:NITER
        fprintf('\rSimulation number: %d noise = %f ', iter, nlevels(nl));
        %title_str = 'AR(1) simulation';

        %noise = makeAR1(tlen,rho);

        % a better way to create AR1 noise?
        if doNewMatrix==1
            noise = randn(tlen,1);
        end

        % note - now we vary SNR by changing the amplitude of the effect
        V = spm_Q(rho,tlen) * noise_amp;
        % This is just a safety measure,to force homogeneous variance:
        V = diag(1./sqrt(diag(V)))*V;
        % W = V^(-1/2) to make inversions easier down the line...
        W = WKfun('mkW',[],V);

        D5W = zeros(size(W));
        for col=1:size(W,2)
            D5W(:,col) = sincsub(W(:,col));
        end
        D5V = zeros(size(V));
        for col=1:size(V,2)
            D5V(:,col) = sincsub(V(:,col));
        end

        noise = V^1/2 *noise;

         % note - now we vary SNR by changing the amplitude of the effect
         signal = X*B ;

        raw = noise + signal;
        orig(iter,:) =  raw';
        t=0:tlen-1;

%         hold on, plot(D3*raw)
%         drawnow

        D5raw = sincsub(raw);

        % method 1: no differencing
        D=D1;
        tmp = pinv(X'*D'*D*X)*X'*D'*D*raw;
        Bhat_raw (iter,:)= tmp';

        if doGLS==1
            % recall that W = V^(-1/2)
            gls_VB_raw = pinv(D*W*X)*D*W* V *W * D'*(pinv(D*W*X))';
        else
            ols_VB_raw = pinv(D*X)*D* V *D'*(pinv(D*X))';
        end

        % method 2: simple subtraction

        xt = raw(1:2:end);
        xc = raw(2:2:end);

        asl = xt - xc;
        simp(iter,:) =  asl';

        D=D2;
        tmp = pinv(X'*D'*D*X)*X'*D'*D*raw;
        %        tmp = pinv(D*W*X)*D*W*raw;
        Bhat_simp (iter,:)= tmp';

        if doGLS==1
            % recall that W = V^(-1/2)
            gls_VB_simp = pinv(D*W*X)*D*W* V *W * D'*(pinv(D*W*X))';
        else
            ols_VB_simp = pinv(D*X)*D* V *D'*(pinv(D*X))';
        end

        % method 3: running subtraction
        asl=zeros(tlen-1,1);
        for n=2:length(raw)
            asl(n-1) = (raw(n)-raw(n-1)) / 2 *(-1)^n;
        end
        run(iter,:)= asl';

        D=D3;
        tmp = pinv(X'*D'*D*X)*X'*D'*D*raw;
        Bhat_run(iter,:)= tmp';

        if doGLS==1
            % recall that W = V^(-1/2)
            gls_VB_run = pinv(D*W*X)*D*W* V *W * D'*(pinv(D*W*X))';
        else
            ols_VB_run = pinv(D*X)*D* V *D'*(pinv(D*X))';
        end

        % method 4: surround subtraction
        asl=zeros(tlen-2,1);
        for n=2:length(raw)-1
            asl(n-1) = (-raw(n+1)+ 2*raw(n) -raw(n-1)) / 4 * (-1)^n;
        end
        sur(iter,:)= asl';

        D=D4;
        tmp = pinv(X'*D'*D*X)*X'*D'*D*raw;
        %        tmp = pinv(D*W*X)*D*W*raw;
        Bhat_sur(iter,:)= tmp';

        if doGLS==1
            % recall that W = V^(-1/2)
            gls_VB_sur = pinv(D*W*X)*D*W* V *W * D'*(pinv(D*W*X))';
        else
            ols_VB_sur = pinv(D*X)*D* V *D'*(pinv(D*X))';
        end

        % method 5: sinc subtraction
        D=D5;
        tmp = pinv(X'*D'*D*X)*X'*D'*D*raw;
        %        tmp = pinv(D*W*X)*D*W*raw;
        Bhat_sinc(iter,:)= tmp';

        if doGLS==1
            % recall that W = V^(-1/2)
            gls_VB_sinc = pinv(D*W*X)*D*W* V *W * D'*(pinv(D*W*X))';
        else
            ols_VB_sinc = pinv(D*X)*D* V *D'*(pinv(D*X))';
        end
        %         tmp = pinv(D5X'*D5X)*D5X'*D5raw;
        %         Bhat_sinc(iter,:)= tmp';
        %
        %         ols_VB_sinc = pinv(D5X)* D5V *(pinv(D5X))';
        %         % recall that W = V^(-1/2)
        %         gls_VB_sinc = pinv(D5W*X)*D5W * V *D5W'*(pinv(D5W*X))';



    end

    % tabulate the stats
    mean_Bhat = ...
        [mean(Bhat_raw,1)  ;
        mean(Bhat_simp,1) ;
        mean(Bhat_run,1) ;
        mean(Bhat_sur,1) ;
        mean(Bhat_sinc,1) ;
        ]


    mean_Bhat_rel = mean_Bhat/mean(Bhat_simp,1);

    %     Monte Carlo
    %   std_Bhat = ...
    %         [std(Bhat_raw,0,1) ;
    %         std(Bhat_simp,0,1) ;
    %         std(Bhat_run,0,1) ;
    %         std(Bhat_sur,0,1) ;
    %         std(Bhat_sinc,0,1) ;
    %         ];
    %

    if doGLS==1
        % now using the GLS formula instead of the brute force ...
        std_Bhat = ...
            [diag(gls_VB_raw)';
            diag(gls_VB_simp)';
            diag(gls_VB_run)';
            diag(gls_VB_sur)';
            diag(gls_VB_sinc)';]
    else
        std_Bhat = ...
            [diag(ols_VB_raw)';
            diag(ols_VB_simp)';
            diag(ols_VB_run)';
            diag(ols_VB_sur)';
            diag(ols_VB_sinc)';
            ]
    end
    std_Bhat_rel = std_Bhat /std(Bhat_simp,0,1);

    eff = 1./std_Bhat;
    eff_rel = eff / eff(2);

    % bias and MSE on the third regressor (activation regressor)
    bias = [ Bhat_raw(:,3) Bhat_simp(:,3) Bhat_run(:,3) Bhat_sur(:,3) Bhat_sinc(:,3) ] ;
    bias = (bias - B(3)) / B(3);
    mean_bias = mean((bias),1)
    mean_bias_rel = mean_bias / mean_bias(2)

    MSE = [ Bhat_raw(:,3)  Bhat_simp(:,3)  Bhat_run(:,3) Bhat_sur(:,3) Bhat_sinc(:,3)] ;
    MSE = (MSE - B(3)).^2 / (B(3))^2;
    mean_MSE = mean((MSE),1);

    % Power calculation given mean, std.dev, and known effect size
    % we'll test the power of estimation of the second B param. in all
    % three types of differencing
    q = zeros(5,3);
    p = ones(5,3);
    Spower=zeros(5,3);
    alpha = 0.05;
    alpha = 0.001;


    df = length(X) - length(B);

    % repeat for each differencing method
    for Dtype=1:5
        % repeat for each of the Bhats
        for c=1:3
            effect_size = abs(B(c));
            %Btmp = mean_Bhat(Dtype, c);
            sigma_B = std_Bhat(Dtype,c);

            % find which value of beta_hat corresponds to the
            % significance level alpha
            % in the null dustribution:
            tcrit =  spm_invTcdf(1-alpha, df);
            bcrit = tcrit * sigma_B;
            q(Dtype,c) = spm_Ncdf(bcrit, effect_size, sigma_B^2);

        end
        bcrit_SNR(Dtype,nl) = bcrit;
    end
    Spower = 1-q

    % and of course over several noise levels:
    Spower_SNR (:,nl) = Spower(:,3);
    eff_SNR (:,nl) = eff(:,3);
    bias_SNR (:,nl) = mean_bias(:,3);
    std_SNR (:,nl) = std_Bhat(:,3);


    % make a little table
    fprintf('\n Power  Efficiency   \n');
    table = [Spower(:,3)  eff(:,3)  ]
    table ./ repmat(table(1,:),5,1)

    % when computing the power for B3 (the activation),
    % we'll try to see what it does over several
    % significance levels
    %     alphas = [0.001 0.005 0.01 0.05 ];
    %     Spower2 = zeros(4, 4); % this variable is for power at different alpha levels.
    %     for a=1:4
    %         effect_size = BB(Dtype , 4);
    %         Btmp = mean_Bhat(Dtype, 4);
    %         sigma_B = std_Bhat(Dtype,3);
    %         tcrit =  spm_invTcdf(1-alphas(a), df);
    %         bcrit = tcrit * sigma_B;
    %         qtmp = spm_Ncdf(bcrit,effect_size, sigma_B);
    %         Spower2(Dtype, a) = 1- qtmp;
    %     end
    save sim_results_OLS.mat ...
        nlevels mean_Bhat MSE Bhat* ...
        mean_bias *_SNR ...
        mean_MSE eff   Spower ...
        mean_Bhat_rel std_Bhat std_Bhat_rel rho NITER




    %
    %     % Now do the DSP stuff to look at the spectra:
    %     % ignore this part when looking at the stats only
    %     if doSpectra==1
    %
    %         if showPlots
    %             hold on
    %             plot(raw,'k'),legend('before noise', 'after noise')
    %             figure
    %             set(gcf,'Position',[1 1 450,450]);
    %         end
    %
    %         % make up frequency vectors in radians and Hz.
    %         w = [-length(raw)/2:length(raw)/2-1]*2*pi/length(raw);
    %         f = w*nyq/(pi);
    %
    %         mforig = mean(fftshift(abs(fft(orig,[],2))),1);
    %
    %         if showPlots
    %             subplot(4,1, 1)
    %             plot(f,mforig), title(title_str,'FontWeight','Bold')
    %             hold on
    %             plot(f,mforig,'k');
    %             axis([0 nyq 0 1.5*max(mforig(end/1.9:end)) ])
    %         end
    %
    %
    %         % simple subtraction:
    %         nyq=1/(2*TR);
    %
    %         mfsimp = mean(fftshift(abs(fft(simp,[],2))),1);
    %         mfsimp = [zeros(1,tlen/4)  mfsimp   zeros(1,ceil(tlen/4))];
    %
    %         if showPlots
    %             subplot(4,1,2)
    %             plot (f, mfsimp), title('Pairwise subtraction','FontWeight', 'Bold')
    %             axis([0 nyq 0 1.5*max(mfsimp(end/2 +1:end)) ])
    %         end
    %
    %
    %         % running subtraction
    %
    %         nyq=1/(2*TR);
    %
    %         mfrun = mean(fftshift(abs(fft(run,[],2))),1);
    %
    %         w = [-length(mfrun)/2:length(mfrun)/2-1]*2*pi/length(mfrun);
    %         f = w*nyq/(pi);
    %
    %         if showPlots
    %             subplot(4,1 ,3)
    %             plot (f,mfrun), title('running subtraction','FontWeight', 'Bold')
    %             axis([0 nyq 0 1.5*max(mfrun(end/2+2:end)) ])
    %             fatlines
    %         end
    %
    %         % fix the original w an f vectors
    %         w = [-length(raw)/2:length(raw)/2-1]*2*pi/length(raw);
    %         f = w*nyq/(pi);
    %
    %
    %         % Surround subtraction
    %
    %         mfsur = mean(fftshift(abs(fft(sur,[],2))),1);
    %         w2 = [ -length(mfsur)/2 : length(mfsur)/2-1 ]  * 2*pi/length(mfsur);
    %         f2 = w2*nyq/pi;
    %
    %         if showPlots
    %             subplot(4,1,4)
    %             plot (f2,mfsur), title('Surround subtraction','FontWeight', 'Bold')
    %             axis([0 nyq 0 2.5*mean(mfsur) ])
    %             fatlines
    %         end
    %
    %         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %         %     Now do the analytical solutions
    %         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %
    %         x = mforig;
    %
    %         % normalize the noise
    %         if showPlots
    %             subplot(4,1,1)
    %             plot(f,abs(x),'r')
    %             legend('data','theoretical')
    %             legend boxoff
    %             fatlines
    %         end
    %         % simple subtraction
    %         ysimp = x.*(1-exp(-i*w));
    %         % aliasing:  Note that it's very important that you apply the
    %         % 1-exp(-i*w) factor before you do the aloasing part...
    %         ysimp_alias = fftshift(ysimp);
    %         ysimp = (ysimp + ysimp_alias)/2;
    %         ysimp(1:end/4) = 0;
    %         ysimp(3*end/4:end) = 0;
    %
    %         if showPlots
    %             subplot(4,1,2)
    %             hold on,
    %             plot(f, abs(ysimp),'r')
    %             fatlines
    %             legend('data','theoretical')
    %             legend boxoff
    %         end
    %
    %         % running subtraction
    %         k = (1-exp(-i*(w+pi)))/2;
    %         yrun = k.*fftshift(x);
    %         if showPlots
    %             subplot(4,1,3)
    %             hold on
    %             plot(f, abs(yrun),'r')
    %             fatlines
    %             legend('data','theoretical')
    %             legend boxoff
    %         end
    %
    %         % surround subtraction:
    %         k = (2 - exp(-i*(w+pi)) - exp(i*(w+pi)))/4;
    %         ysur = k.*fftshift(x);
    %         if showPlots
    %             subplot(4,1,4)
    %             hold on
    %             plot(nyq*w/pi, abs(ysur),'r')
    %             fatlines
    %         end
    %         %legend('data','theoretical')
    %         %legend boxoff
    %
    %
    %     end

end



if showPlots==1
    figure
    plot(1./nlevels, Spower_SNR')
    legend('D1','D2','D3','D4','D5')
    xlabel('SNR'), ylabel('Power')
    axis tight, fatlines, dofontsize(16)
    axis([0 0.5 0.5 1.1])

    figure
    plot(1./nlevels, eff_SNR);%'.*repmat(nlevels',1,4))
    %legend('D1','D2','D3','D4')
    xlabel('SNR'), ylabel('Efficiency')
    legend('D1','D2','D3','D4','D5')
    axis tight, fatlines, dofontsize(16)

    figure
    plot(1./nlevels, bias_SNR')
    %legend('D1','D2','D3','D4')
    xlabel('SNR'), ylabel('BIAS')
    legend('D1','D2','D3','D4','D5')
    axis tight, fatlines, dofontsize(16)

    figure
    plot(1./nlevels, std_SNR);%'./repmat(nlevels',1,4))
    %legend('D1','D2','D3','D4')
    xlabel('SNR'), ylabel('Estimate Variance')
    legend('D1','D2','D3','D4','D5')
    axis tight, fatlines, dofontsize(16)
end
\end{verbatim}

\color{lightgray} \begin{verbatim}
B =

  100.0000
   -1.0000
   -0.5000

Noise level ...  0.200000 Simulation number: 1 noise = 0.200000 
mean_Bhat =

  100.0036   -1.0104   -0.4471
         0   -1.0156   -0.4286
    0.0000   -1.0111   -0.4380
   -0.0000   -1.0109   -0.4354
   -0.0000   -1.0101   -0.4437


std_Bhat =

    0.0047    0.0060    0.0281
         0    0.0065    0.0336
         0    0.0060    0.0280
         0    0.0060    0.0284
    0.0000    0.0060    0.0282


mean_bias =

   -0.1059   -0.1429   -0.1239   -0.1293   -0.1127


mean_bias_rel =

    0.7410    1.0000    0.8674    0.9051    0.7887


Spower =

     1     1     1
   NaN     1     1
   NaN     1     1
   NaN     1     1
     1     1     1


 Power  Efficiency   

table =

    1.0000   35.6086
    1.0000   29.7402
    1.0000   35.7569
    1.0000   35.2027
    1.0000   35.4466


ans =

    1.0000    1.0000
    1.0000    0.8352
    1.0000    1.0042
    1.0000    0.9886
    1.0000    0.9954

Noise level ...  0.500000 Simulation number: 1 noise = 0.500000 
mean_Bhat =

   99.9999   -1.0018   -0.4916
         0   -1.0039   -0.4842
    0.0000   -1.0023   -0.4878
   -0.0000   -1.0021   -0.4874
   -0.0000   -1.0021   -0.4890


std_Bhat =

    0.0074    0.0095    0.0444
         0    0.0102    0.0532
         0    0.0095    0.0442
         0    0.0095    0.0449
    0.0000    0.0095    0.0446


mean_bias =

   -0.0168   -0.0316   -0.0244   -0.0251   -0.0221


mean_bias_rel =

    0.5340    1.0000    0.7731    0.7966    0.6991


Spower =

    1.0000    1.0000    1.0000
       NaN    1.0000    1.0000
       NaN    1.0000    1.0000
       NaN    1.0000    1.0000
    1.0000    1.0000    1.0000


 Power  Efficiency   

table =

    1.0000   22.5209
    1.0000   18.8093
    1.0000   22.6147
    1.0000   22.2642
    1.0000   22.4184


ans =

    1.0000    1.0000
    1.0000    0.8352
    1.0000    1.0042
    1.0000    0.9886
    1.0000    0.9954

Noise level ...  1.000000 Simulation number: 1 noise = 1.000000 
mean_Bhat =

   99.9992   -1.0002   -0.4993
         0   -1.0015   -0.4945
    0.0000   -1.0007   -0.4965
   -0.0000   -1.0005   -0.4965
   -0.0000   -1.0007   -0.4968


std_Bhat =

    0.0105    0.0134    0.0628
         0    0.0145    0.0752
         0    0.0134    0.0625
         0    0.0135    0.0635
    0.0000    0.0135    0.0631


mean_bias =

   -0.0015   -0.0110   -0.0069   -0.0069   -0.0064


mean_bias_rel =

    0.1351    1.0000    0.6319    0.6318    0.5817


Spower =

    1.0000    1.0000    1.0000
       NaN    1.0000    0.9998
       NaN    1.0000    1.0000
       NaN    1.0000    1.0000
    1.0000    1.0000    1.0000


 Power  Efficiency   

table =

    1.0000   15.9247
    0.9998   13.3002
    1.0000   15.9910
    1.0000   15.7431
    1.0000   15.8522


ans =

    1.0000    1.0000
    0.9998    0.8352
    1.0000    1.0042
    1.0000    0.9886
    1.0000    0.9954

Noise level ...  3.000000 Simulation number: 1 noise = 3.000000 
mean_Bhat =

   99.9985   -0.9995   -0.5020
         0   -1.0011   -0.4962
    0.0000   -1.0004   -0.4982
   -0.0000   -1.0002   -0.4983
   -0.0000   -1.0005   -0.4982


std_Bhat =

    0.0182    0.0233    0.1088
         0    0.0250    0.1302
         0    0.0232    0.1083
         0    0.0234    0.1100
    0.0000    0.0233    0.1093


mean_bias =

    0.0039   -0.0076   -0.0036   -0.0034   -0.0036


mean_bias_rel =

   -0.5111    1.0000    0.4701    0.4407    0.4704


Spower =

    1.0000    1.0000    0.9298
       NaN    1.0000    0.7633
       NaN    1.0000    0.9324
       NaN    1.0000    0.9225
    1.0000    1.0000    0.9270


 Power  Efficiency   

table =

    0.9298    9.1941
    0.7633    7.6789
    0.9324    9.2324
    0.9225    9.0893
    0.9270    9.1523


ans =

    1.0000    1.0000
    0.8209    0.8352
    1.0027    1.0042
    0.9921    0.9886
    0.9969    0.9954

Noise level ...  5.000000 Simulation number: 1 noise = 5.000000 
mean_Bhat =

   99.9969   -0.9984   -0.5056
         0   -1.0011   -0.4961
    0.0000   -1.0005   -0.4987
   -0.0000   -1.0001   -0.4989
   -0.0000   -1.0006   -0.4984


std_Bhat =

    0.0235    0.0300    0.1404
         0    0.0323    0.1681
         0    0.0300    0.1398
         0    0.0302    0.1420
    0.0000    0.0301    0.1411


mean_bias =

    0.0111   -0.0077   -0.0027   -0.0022   -0.0033


mean_bias_rel =

   -1.4428    1.0000    0.3460    0.2899    0.4228


Spower =

    1.0000    1.0000    0.6694
       NaN    1.0000    0.4410
       NaN    1.0000    0.6748
       NaN    1.0000    0.6546
    1.0000    1.0000    0.6635


 Power  Efficiency   

table =

    0.6694    7.1217
    0.4410    5.9480
    0.6748    7.1514
    0.6546    7.0405
    0.6635    7.0893


ans =

    1.0000    1.0000
    0.6587    0.8352
    1.0080    1.0042
    0.9778    0.9886
    0.9912    0.9954

Noise level ...  7.000000 Simulation number: 1 noise = 7.000000 
mean_Bhat =

   99.9927   -0.9958   -0.5138
         0   -1.0010   -0.4955
    0.0000   -1.0008   -0.4986
   -0.0000   -1.0000   -0.4990
   -0.0000   -1.0011   -0.4977


std_Bhat =

    0.0278    0.0356    0.1661
         0    0.0382    0.1989
         0    0.0355    0.1655
         0    0.0357    0.1681
    0.0000    0.0356    0.1669


mean_bias =

    0.0276   -0.0090   -0.0028   -0.0020   -0.0046


mean_bias_rel =

   -3.0688    1.0000    0.3061    0.2269    0.5056


Spower =

    1.0000    1.0000    0.4550
       NaN    1.0000    0.2713
       NaN    1.0000    0.4600
       NaN    1.0000    0.4414
    1.0000    1.0000    0.4496


 Power  Efficiency   

table =

    0.4550    6.0190
    0.2713    5.0270
    0.4600    6.0440
    0.4414    5.9503
    0.4496    5.9916


ans =

    1.0000    1.0000
    0.5962    0.8352
    1.0109    1.0042
    0.9702    0.9886
    0.9881    0.9954

Noise level ...  9.000000 Simulation number: 1 noise = 9.000000 
mean_Bhat =

   99.9813   -0.9882   -0.5364
         0   -0.9996   -0.4962
    0.0000   -1.0018   -0.4981
   -0.0000   -1.0000   -0.4986
   -0.0000   -1.0028   -0.4957


std_Bhat =

    0.0315    0.0403    0.1884
         0    0.0434    0.2256
         0    0.0402    0.1876
         0    0.0405    0.1906
    0.0000    0.0404    0.1892


mean_bias =

    0.0728   -0.0076   -0.0038   -0.0028   -0.0087


mean_bias_rel =

   -9.5882    1.0000    0.4997    0.3696    1.1456


Spower =

    1.0000    1.0000    0.3198
       NaN    1.0000    0.1825
       NaN    1.0000    0.3237
       NaN    1.0000    0.3090
    1.0000    1.0000    0.3154


 Power  Efficiency   

table =

    0.3198    5.3082
    0.1825    4.4334
    0.3237    5.3303
    0.3090    5.2477
    0.3154    5.2841


ans =

    1.0000    1.0000
    0.5708    0.8352
    1.0124    1.0042
    0.9664    0.9886
    0.9865    0.9954

Noise level ...  20.000000 Simulation number: 1 noise = 20.000000 
mean_Bhat =

   99.9287   -0.9525   -0.6403
         0   -0.9892   -0.5113
    0.0000   -1.0069   -0.4959
   -0.0000   -1.0001   -0.4961
   -0.0000   -1.0108   -0.4867


std_Bhat =

    0.0470    0.0601    0.2808
         0    0.0647    0.3362
         0    0.0600    0.2797
         0    0.0604    0.2841
    0.0000    0.0602    0.2821


mean_bias =

    0.2805    0.0225   -0.0082   -0.0077   -0.0267


mean_bias_rel =

   12.4609    1.0000   -0.3638   -0.3429   -1.1855


Spower =

    1.0000    1.0000    0.0898
       NaN    1.0000    0.0510
       NaN    1.0000    0.0910
       NaN    1.0000    0.0865
    1.0000    1.0000    0.0885


 Power  Efficiency   

table =

    0.0898    3.5609
    0.0510    2.9740
    0.0910    3.5757
    0.0865    3.5203
    0.0885    3.5447


ans =

    1.0000    1.0000
    0.5677    0.8352
    1.0135    1.0042
    0.9639    0.9886
    0.9854    0.9954


ans =

    16


ans =

    16


ans =

    16


ans =

    16

\end{verbatim} \color{black}

\includegraphics [width=4in]{ASLnoise_eff_01.epsc2}

\includegraphics [width=4in]{ASLnoise_eff_02.epsc2}

\includegraphics [width=4in]{ASLnoise_eff_03.epsc2}

\includegraphics [width=4in]{ASLnoise_eff_04.epsc2}



\end{document}
    
